{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#      VIRTUALFIT: FINAL COLAB BACKEND SERVER (Corrected)\n",
        "# ==============================================================================\n",
        "# This version removes the faulty debug check and relies on the model's native\n",
        "# data loading, which is already proven to work with your file structure.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- SEGMENT 0: SERVER & DEPENDENCY IMPORTS ---\n",
        "print(\"--- SEGMENT 0: INSTALLING LIBRARIES & IMPORTS ---\")\n",
        "!pip install -q \"fastapi[all]\" pyngrok\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import nest_asyncio\n",
        "from google.colab import drive\n",
        "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
        "from fastapi.responses import FileResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- SEGMENT 1: ENVIRONMENT SETUP ---\n",
        "print(\"\\n--- SEGMENT 1: SETTING UP THE ENVIRONMENT ---\")\n",
        "drive.mount('/content/drive')\n",
        "PROJECT_BASE_DIR = '/content/drive/MyDrive/VirtualFIT_Models'\n",
        "DRIVE_ASSETS_DIR = os.path.join(PROJECT_BASE_DIR, 'viton_hd_zips')\n",
        "DRIVE_TEST_DATA_PATH = os.path.join(DRIVE_ASSETS_DIR, 'test')\n",
        "DRIVE_CHECKPOINTS_ZIP_PATH = os.path.join(DRIVE_ASSETS_DIR, 'checkpoints.zip')\n",
        "VITON_HD_CLONE_DIR = '/content/VITON_HD_repo'\n",
        "VITON_HD_DATA_DIR = os.path.join(VITON_HD_CLONE_DIR, 'datasets')\n",
        "VITON_HD_TEST_DIR = os.path.join(VITON_HD_DATA_DIR, 'test')\n",
        "VITON_HD_CHECKPOINTS_DIR = os.path.join(VITON_HD_CLONE_DIR, 'checkpoints')\n",
        "\n",
        "os.chdir('/content')\n",
        "!rm -rf {VITON_HD_CLONE_DIR}\n",
        "!git clone -q https://github.com/shadow2496/VITON-HD.git {VITON_HD_CLONE_DIR}\n",
        "os.chdir(VITON_HD_CLONE_DIR)\n",
        "\n",
        "os.makedirs(VITON_HD_TEST_DIR, exist_ok=True)\n",
        "os.makedirs(VITON_HD_CHECKPOINTS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\nCopying base assets from: {DRIVE_ASSETS_DIR}\")\n",
        "if os.path.isdir(DRIVE_TEST_DATA_PATH):\n",
        "    # This command copies all your subdirectories with their existing names\n",
        "    !cp -r \"{DRIVE_TEST_DATA_PATH}\"/* \"{VITON_HD_TEST_DIR}/\"\n",
        "if os.path.exists(DRIVE_CHECKPOINTS_ZIP_PATH):\n",
        "    !cp \"{DRIVE_CHECKPOINTS_ZIP_PATH}\" \"{VITON_HD_CHECKPOINTS_DIR}/\"\n",
        "    !unzip -o -q \"{os.path.join(VITON_HD_CHECKPOINTS_DIR, 'checkpoints.zip')}\" -d \"{VITON_HD_CHECKPOINTS_DIR}/\"\n",
        "    if os.path.exists(os.path.join(VITON_HD_CHECKPOINTS_DIR, 'checkpoints')):\n",
        "        !mv {VITON_HD_CHECKPOINTS_DIR}/checkpoints/* {VITON_HD_CHECKPOINTS_DIR}/ && rm -rf {VITON_HD_CHECKPOINTS_DIR}/checkpoints\n",
        "\n",
        "# --- SEGMENT 2: PATCH SCRIPT & INSTALL DEPENDENCIES ---\n",
        "print(\"\\n--- SEGMENT 2: PATCHING SCRIPT & INSTALLING DEPENDENCIES ---\")\n",
        "!pip install -q torch torchvision opencv-python torchgeometry\n",
        "TEST_PY_PATH = os.path.join(VITON_HD_CLONE_DIR, 'test.py')\n",
        "corrected_test_py_content = r\"\"\"\n",
        "import argparse, os, torch; from torch import nn; from torch.nn import functional as F; import torchgeometry as tgm\n",
        "from datasets import VITONDataset, VITONDataLoader; from networks import SegGenerator, GMM, ALIASGenerator; from utils import gen_noise, load_checkpoint, save_images\n",
        "def get_opt():\n",
        "    parser = argparse.ArgumentParser(); parser.add_argument('--name', type=str, required=True); parser.add_argument('-b', '--batch_size', type=int, default=1); parser.add_argument('-j', '--workers', type=int, default=1)\n",
        "    parser.add_argument('--load_height', type=int, default=1024); parser.add_argument('--load_width', type=int, default=768); parser.add_argument('--shuffle', action='store_true')\n",
        "    parser.add_argument('--dataset_dir', type=str, default='./datasets/'); parser.add_argument('--dataset_mode', type=str, default='test'); parser.add_argument('--dataset_list', type=str, default='test_pairs.txt')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints/'); parser.add_argument('--save_dir', type=str, default='./results/'); parser.add_argument('--display_freq', type=int, default=1)\n",
        "    parser.add_argument('--seg_checkpoint', type=str, default='seg_final.pth'); parser.add_argument('--gmm_checkpoint', type=str, default='gmm_final.pth'); parser.add_argument('--alias_checkpoint', type=str, default='alias_final.pth')\n",
        "    parser.add_argument('--semantic_nc', type=int, default=13); parser.add_argument('--init_type', choices=['normal', 'xavier', 'xavier_uniform', 'kaiming', 'orthogonal', 'none'], default='xavier')\n",
        "    parser.add_argument('--init_variance', type=float, default=0.02); parser.add_argument('--grid_size', type=int, default=5); parser.add_argument('--norm_G', type=str, default='spectralaliasinstance')\n",
        "    parser.add_argument('--ngf', type=int, default=64); parser.add_argument('--num_upsampling_layers', choices=['normal', 'more', 'most'], default='most'); opt = parser.parse_args(); return opt\n",
        "def test(opt, seg, gmm, alias, device):\n",
        "    up = nn.Upsample(size=(opt.load_height, opt.load_width), mode='bilinear'); gauss = tgm.image.GaussianBlur((15, 15), (3, 3)).to(device)\n",
        "    test_dataset = VITONDataset(opt); test_loader = VITONDataLoader(opt, test_dataset)\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader.data_loader):\n",
        "            img_names = inputs['img_name']; c_names = inputs['c_name']['unpaired']; img_agnostic = inputs['img_agnostic'].to(device)\n",
        "            parse_agnostic = inputs['parse_agnostic'].to(device); pose = inputs['pose'].to(device); c = inputs['cloth']['unpaired'].to(device); cm = inputs['cloth_mask']['unpaired'].to(device)\n",
        "            parse_agnostic_down = F.interpolate(parse_agnostic, size=(256, 192), mode='bilinear'); pose_down = F.interpolate(pose, size=(256, 192), mode='bilinear')\n",
        "            c_masked_down = F.interpolate(c * cm, size=(256, 192), mode='bilinear'); cm_down = F.interpolate(cm, size=(256, 192), mode='bilinear')\n",
        "            seg_input = torch.cat((cm_down, c_masked_down, parse_agnostic_down, pose_down, gen_noise(cm_down.size()).to(device)), dim=1)\n",
        "            parse_pred_down = seg(seg_input); parse_pred = gauss(up(parse_pred_down)); parse_pred = parse_pred.argmax(dim=1)[:, None]\n",
        "            parse_old = torch.zeros(parse_pred.size(0), 13, opt.load_height, opt.load_width, dtype=torch.float).to(device); parse_old.scatter_(1, parse_pred, 1.0)\n",
        "            labels = {0:['background',[0]], 1:['paste',[2,4,7,8,9,10,11]], 2:['upper',[3]], 3:['hair',[1]], 4:['left_arm',[5]], 5:['right_arm',[6]], 6:['noise',[12]]}\n",
        "            parse = torch.zeros(parse_pred.size(0), 7, opt.load_height, opt.load_width, dtype=torch.float).to(device)\n",
        "            for j in range(len(labels)):\n",
        "                for label in labels[j][1]: parse[:, j] += parse_old[:, label]\n",
        "            agnostic_gmm = F.interpolate(img_agnostic, size=(256, 192), mode='nearest'); parse_cloth_gmm = F.interpolate(parse[:, 2:3], size=(256, 192), mode='nearest')\n",
        "            pose_gmm = F.interpolate(pose, size=(256, 192), mode='nearest'); c_gmm = F.interpolate(c, size=(256, 192), mode='nearest'); gmm_input = torch.cat((parse_cloth_gmm, pose_gmm, agnostic_gmm), dim=1)\n",
        "            _, warped_grid = gmm(gmm_input, c_gmm); warped_c = F.grid_sample(c, warped_grid, padding_mode='border'); warped_cm = F.grid_sample(cm, warped_grid, padding_mode='border')\n",
        "            misalign_mask = parse[:, 2:3] - warped_cm; misalign_mask[misalign_mask < 0.0] = 0.0\n",
        "            parse_div = torch.cat((parse, misalign_mask), dim=1); parse_div[:, 2:3] -= misalign_mask\n",
        "            output = alias(torch.cat((img_agnostic, pose, warped_c), dim=1), parse, parse_div, misalign_mask)\n",
        "            unpaired_names = ['{}_{}'.format(img_name.split('_')[0], c_name) for img_name, c_name in zip(img_names, c_names)]\n",
        "            save_images(output, unpaired_names, os.path.join(opt.save_dir, opt.name))\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); opt = get_opt(); print(opt)\n",
        "    if not os.path.exists(os.path.join(opt.save_dir, opt.name)): os.makedirs(os.path.join(opt.save_dir, opt.name))\n",
        "    seg = SegGenerator(opt, input_nc=opt.semantic_nc + 8, output_nc=opt.semantic_nc); gmm = GMM(opt, inputA_nc=7, inputB_nc=3)\n",
        "    opt.semantic_nc = 7; alias = ALIASGenerator(opt, input_nc=9); opt.semantic_nc = 13\n",
        "    load_checkpoint(seg, os.path.join(opt.checkpoint_dir, opt.seg_checkpoint)); load_checkpoint(gmm, os.path.join(opt.checkpoint_dir, opt.gmm_checkpoint)); load_checkpoint(alias, os.path.join(opt.checkpoint_dir, opt.alias_checkpoint))\n",
        "    seg.to(device).eval(); gmm.to(device).eval(); alias.to(device).eval()\n",
        "    test(opt, seg, gmm, alias, device)\n",
        "if __name__ == '__main__': main()\n",
        "\"\"\"\n",
        "with open(TEST_PY_PATH, 'w') as f:\n",
        "    f.write(corrected_test_py_content.replace('F.grid_sample(c, warped_grid, padding_mode=\\'border\\')', 'F.grid_sample(c, warped_grid, padding_mode=\\'border\\', align_corners=False)').replace('F.grid_sample(cm, warped_grid, padding_mode=\\'border\\')', 'F.grid_sample(cm, warped_grid, padding_mode=\\'border\\', align_corners=False)').replace(\"mode='bilinear'\", \"mode='bilinear', align_corners=False\"))\n",
        "print(\"âœ… Setup Complete. Ready to start server.\")\n",
        "\n",
        "# --- SEGMENT 3: START THE SERVER ---\n",
        "print(\"\\n--- SEGMENT 3: STARTING THE FASTAPI SERVER ---\")\n",
        "app = FastAPI()\n",
        "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
        "\n",
        "@app.post(\"/run-inference/\")\n",
        "async def run_inference(person_image: UploadFile = File(...), cloth_image: UploadFile = File(...)):\n",
        "    person_filename = person_image.filename\n",
        "    cloth_filename = cloth_image.filename\n",
        "    print(f\"âœ… Received request: Person='{person_filename}', Cloth='{cloth_filename}'\")\n",
        "\n",
        "    # 1. Save the uploaded files\n",
        "    with open(os.path.join(VITON_HD_TEST_DIR, 'image', person_filename), \"wb\") as buffer: shutil.copyfileobj(person_image.file, buffer)\n",
        "    with open(os.path.join(VITON_HD_TEST_DIR, 'cloth', cloth_filename), \"wb\") as buffer: shutil.copyfileobj(cloth_image.file, buffer)\n",
        "\n",
        "    # 2. Overwrite test_pairs.txt\n",
        "    pairs_file_path = os.path.join(VITON_HD_DATA_DIR, \"test_pairs.txt\")\n",
        "    with open(pairs_file_path, 'w') as f: f.write(f\"{person_filename} {cloth_filename}\")\n",
        "    print(\"âœ… test_pairs.txt overwritten. Handing off to the model...\")\n",
        "\n",
        "    # 3. Run the inference commands\n",
        "    try:\n",
        "        # This now behaves exactly like your simple script, trusting the model's\n",
        "        # internal dataloader to find the files in the folders you provided.\n",
        "        subprocess.run([\"python\", \"test.py\", \"--name\", \"alias_stage\", \"--dataset_list\", \"test_pairs.txt\"], check=True, cwd=VITON_HD_CLONE_DIR, capture_output=True, text=True)\n",
        "        print(\"âœ… Inference complete.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # If it fails now, the error is from the model itself.\n",
        "        print(f\"âŒ MODEL ERROR: {e.stderr}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Model inference failed: {e.stderr}\")\n",
        "\n",
        "    # 4. Find and return the result image\n",
        "    results_dir = os.path.join(VITON_HD_CLONE_DIR, 'results', 'alias_stage')\n",
        "    person_id = person_filename.split('_')[0]\n",
        "    expected_result_filename = f\"{person_id}_{cloth_filename}\"\n",
        "    result_path = os.path.join(results_dir, expected_result_filename)\n",
        "    if not os.path.exists(result_path):\n",
        "        all_results = os.listdir(results_dir)\n",
        "        if all_results: result_path = os.path.join(results_dir, all_results[0])\n",
        "        else: raise HTTPException(status_code=404, detail=\"Result image not found.\")\n",
        "\n",
        "    return FileResponse(result_path, media_type=\"image/jpeg\")\n",
        "\n",
        "# --- SEGMENT 4: LAUNCH THE SERVER ---\n",
        "ngrok_authtoken = \"31D0MlOfXiyJnvVtIMmaYalNgjl_7mRbsniXVPb1c9CrNnm2J\"\n",
        "ngrok.set_auth_token(ngrok_authtoken)\n",
        "http_tunnel = ngrok.connect(8000)\n",
        "print(\"===================================================================\")\n",
        "print(\"                    âœ… Your Colab Backend is LIVE! âœ…\")\n",
        "print(f\"ðŸ”— Public URL: {http_tunnel.public_url}\")\n",
        "print(\"\\nACTION: Use this URL in your project's backend to send requests.\")\n",
        "print(\"===================================================================\")\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzrf_eeT_l7J",
        "outputId": "791a6547-afb4-46e3-a0cb-2d83a508ae71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SEGMENT 0: INSTALLING LIBRARIES & IMPORTS ---\n",
            "\n",
            "--- SEGMENT 1: SETTING UP THE ENVIRONMENT ---\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Copying base assets from: /content/drive/MyDrive/VirtualFIT_Models/viton_hd_zips\n",
            "\n",
            "--- SEGMENT 2: PATCHING SCRIPT & INSTALLING DEPENDENCIES ---\n",
            "âœ… Setup Complete. Ready to start server.\n",
            "\n",
            "--- SEGMENT 3: STARTING THE FASTAPI SERVER ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [23430]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================================\n",
            "                    âœ… Your Colab Backend is LIVE! âœ…\n",
            "ðŸ”— Public URL: https://208f3e471f84.ngrok-free.app\n",
            "\n",
            "ACTION: Use this URL in your project's backend to send requests.\n",
            "===================================================================\n",
            "âœ… Received request: Person='00067_00.jpg', Cloth='07118_00.jpg'\n",
            "âœ… test_pairs.txt overwritten. Handing off to the model...\n",
            "âœ… Inference complete.\n",
            "INFO:     103.106.57.16:0 - \"POST /run-inference/ HTTP/1.1\" 200 OK\n",
            "âœ… Received request: Person='00112_00.jpg', Cloth='08255_00.jpg'\n",
            "âœ… test_pairs.txt overwritten. Handing off to the model...\n",
            "âœ… Inference complete.\n",
            "INFO:     103.106.57.16:0 - \"POST /run-inference/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [23430]\n"
          ]
        }
      ]
    }
  ]
}